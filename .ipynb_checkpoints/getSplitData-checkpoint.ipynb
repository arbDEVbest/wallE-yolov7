{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\"> Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\"> Work directory must be TACO\n",
    "### ` If not `\n",
    "> ```bash\n",
    "> cd TACO/\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/TACO'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\">Download TACO datasets \n",
    "> Use terminal and for all instructions read REAME.md\n",
    "> ```bash\n",
    "> python download.py\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\">When the datasets donwloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/TACO/detector\n"
     ]
    }
   ],
   "source": [
    "cd detector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python split_dataset.py --dataset_dir ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`` The annotations has splited to 30 batchs of json files.``\n",
    "> * 10 Train\n",
    "> * 10 Test\n",
    "> * 10 val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Return to Taco directory `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`` Assign a variable for taco datasets ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/notebooks/TACO/data')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TACO = Path.cwd()/\"TACO\"/\"data\"\n",
    "DATA_TACO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mTACO\u001b[0m/  \u001b[01;34mdatasets\u001b[0m/  \u001b[01;34myolov7\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset mapping directories of yolov7\n",
    "1. > Create images directories of train, test and val directories, and the file json > annotation for each split directory from Taco files json annotations\n",
    "2.  > Create labels directories of train, test and val directories, does labels contains txt files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build images directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_images_and_json_file(path_dir_annotations:Path, list_dir_split:List[str]):\n",
    "    \"\"\"Build a mapping directeries of yalov7. \n",
    "    This funtion create images directories of train, test and val directeries.\n",
    "    Also it create the annotations json files of splits directories\n",
    "\n",
    "    Args:\n",
    "        path_dir_annotations (Path): It's a path of annotations directory (e.g TACO/data/annotations)\n",
    "        list_dir_split (List[str]): split's names list (e.g [\"train\",\"test\",\"val\"])\n",
    "    \"\"\"\n",
    "    # project directory\n",
    "    project_dir = Path.cwd()\n",
    "    print(project_dir)\n",
    "    # create a new data directory contains the splits directories train, test and val\n",
    "    datasets = project_dir/\"datasets\"\n",
    "    datasets.mkdir(exist_ok=True)\n",
    "    # Get a list all of json files annatations\n",
    "    file_json = [f for f in path_dir_annotations.parent.iterdir() if f.is_file() and str(f).endswith(\".json\")]\n",
    "    # Get a list all of images batchs directories\n",
    "    dir_imgs = [d for d in path_dir_annotations.iterdir() if d.is_dir()]\n",
    "    # Get a parent of images directories\n",
    "    path_dir_image_batch=dir_imgs[0].parent\n",
    "    print(path_dir_image_batch)\n",
    "    # Iterate over the list that contains the names of created folders\n",
    "    for annot in list_dir_split:\n",
    "        print(annot)\n",
    "        #Dictionary of the new specific annotations json file\n",
    "        dict_js = {}\n",
    "        # Name of split folder (e.g \"datasets/train\")\n",
    "        print(datasets)\n",
    "        dir_split = datasets/annot\n",
    "        dir_split.mkdir(exist_ok=True)\n",
    "        images_dir = dir_split/\"images\"\n",
    "        images_dir.mkdir(exist_ok=True)\n",
    "        # Get a list all of json files annotations of specific split directory\n",
    "        file_split = [f for f in file_json if f.is_file() and str(f).__contains__(annot)]\n",
    "\n",
    "        # Iterate over annotations json files list\n",
    "        for fic in file_split:\n",
    "            with open(fic,\"r\", encoding=\"utf-8\") as f:\n",
    "                json_load = json.load(f)\n",
    "            # update dictionary with the contains annotations json files \n",
    "            dict_js.update(json_load)\n",
    "        for el in dict_js[\"images\"]:\n",
    "            file_name = el[\"file_name\"]\n",
    "            path_img = path_dir_image_batch/file_name\n",
    "            \n",
    "            file_out = \"_\".join(file_name.split(\"/\"))\n",
    "            output_file = dir_split/\"images\"/file_out\n",
    "            el[\"file_name\"]=f\"{annot}/images/{file_out}\"\n",
    "            if not output_file.exists():\n",
    "                output_file.write_bytes(path_img.read_bytes())\n",
    "            \n",
    "        with open(dir_split/f\"annotations-{annot}.json\",\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict_js,f,indent=4)\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Function build_split_images_and_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a variable directories' split names\n",
    "split_list_dir = [\"train\",\"test\",\"val\"]\n",
    "# Assign variable Annotations directory\n",
    "annotations = DATA_TACO/\"annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n",
      "/notebooks/TACO/data/annotations\n",
      "train\n",
      "/notebooks/datasets\n",
      "test\n",
      "/notebooks/datasets\n",
      "val\n",
      "/notebooks/datasets\n"
     ]
    }
   ],
   "source": [
    "# Run Function\n",
    "build_split_images_and_json_file(annotations,split_list_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if the image transfer went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function checking count of each images directories as well\n",
    "def check_count_imgs_in_split_dir(list_path_images_split_dir:List[Path]):\n",
    "    for img in list_path_images_split_dir:\n",
    "        name_dir = img.parent.name\n",
    "        list_path = [f for f in img.iterdir() if f.is_file()]\n",
    "        print(f\"count images of {name_dir} folder is : {len(list_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DATA_TACO.parent.parent/\"datasets\"\n",
    "train_images_dir = datasets/\"train\"/\"images\"\n",
    "test_images_dir = datasets/\"test\"/\"images\"\n",
    "val_images_dir = datasets/\"val\"/\"images\"\n",
    "list_split_pathImgs = [train_images_dir,test_images_dir,val_images_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count images of train folder is : 1200\n",
      "count images of test folder is : 150\n",
      "count images of val folder is : 150\n"
     ]
    }
   ],
   "source": [
    "check_count_imgs_in_split_dir(list_split_pathImgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the labels directory for each split directory \n",
    "In labels directory we are creating txt files for each image segmentations.\n",
    "These files contains the coordinates of the frames of each waste in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_bbox(df):\n",
    "        bbox = df[\"bbox\"].values[0]\n",
    "        \n",
    "        W = df[\"width\"].values[0]\n",
    "        H = df[\"height\"].values[0]\n",
    "        x = bbox[0] \n",
    "        y = bbox[1] \n",
    "        w = bbox[2]\n",
    "        h = bbox[3]\n",
    "        X = np.round((x + w/2)/W,6)\n",
    "        Y = np.round((y + h/2)/H,6)\n",
    "        wn = np.round(w/W)\n",
    "        hn = np.round(h/H,6)\n",
    "        return [X,Y,wn, hn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_segmentation(seg_values):\n",
    "    seg_value = seg_values['segmentation'].values[0][0]\n",
    "    width = seg_values[\"width\"].values\n",
    "    height = seg_values[\"height\"].values\n",
    "    \n",
    "    list_seg = []\n",
    "    \n",
    "    for i, v in enumerate(seg_value):\n",
    "        if i % 2:\n",
    "            \n",
    "            y = np.round(float(v)/float(height[0]),6)\n",
    "            \n",
    "            list_seg.append(y)\n",
    "        else:\n",
    "            \n",
    "            #print(\"width ===>\",type(width[0]))\n",
    "            x = np.round(float(v)/float(width[0]),6)\n",
    "            list_seg.append(x)\n",
    "    return list_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\repositoryProf\\Project_E2\\TACO\\data\\annotations_0_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            annotates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Create DataFrame from data json file\n",
    "images = pd.DataFrame(annotates[\"images\"], columns=[\"id\",\"file_name\",\"width\",\"height\"])\n",
    "images.rename(columns={\"id\":\"image_id\"}, inplace=True)\n",
    "annot = pd.DataFrame(annotates[\"annotations\"], columns=[\"id\",\"image_id\",\"category_id\",\"segmentation\",\"bbox\"])\n",
    "cat = pd.DataFrame(annotates[\"categories\"]).rename(columns={\"id\":\"category_id\"}).sort_values(by=\"category_id\", ascending=True)\n",
    "df = annot.merge(images)\n",
    "df = df.merge(cat)\n",
    "cat_index = cat.supercategory.unique()\n",
    "df.supercategory_id = np.nan\n",
    "for i,v in enumerate(cat_index):\n",
    "    df.loc[df['supercategory']==v, \"supercategory_id\"]= i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'image_id', 'category_id', 'segmentation', 'bbox', 'file_name',\n",
       "       'width', 'height', 'supercategory', 'name', 'supercategory_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1537, 3264, 1824, ..., 2268, 2448, 2448], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"width\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_h = df[df[\"image_id\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[928.0, 1876.0, 938.0, 1856.0, 968.0, 1826.0,...\n",
       "Name: segmentation, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter_df = df[df[\"image_id\"]==1][[\"image_id\",\"category_id\",\"segmentation\",\"bbox\",\"file_name\",\"width\",\"height\",\"supercategory_id\"]]\n",
    "# filter_df.iloc[0:1,:][\"segmentation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq = df[\"image_id\"].unique()\n",
    "# for un in uniq:\n",
    "#     filter_df = df[df[\"image_id\"]==un]\n",
    "#     pprint(filter_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels_txt_with_segmentations(data_path_dir:Path,names_dir:List[str]):\n",
    "    \"\"\"Create a folders for each split directory (e.g train, test, val), in each folder we create labels folder\n",
    "    These labels folders contains txt files \n",
    "\n",
    "    Args:\n",
    "        data_path_dir (Path): Path of dataset directory for all of mapping data for yolov7\n",
    "        names_dir (List[str]): The list names of each splits directories (e.g [\"train\",\"test\",\"val\"])\n",
    "    \"\"\"\n",
    "    #Iterate over names split diretories list\n",
    "    for name in names_dir:\n",
    "        path_annotations_dir = datasets/name\n",
    "\n",
    "        #Get transformed annotations json file in this directory\n",
    "        path_annotations = [f for f in path_annotations_dir.iterdir() if str(f).endswith(\".json\")][0]\n",
    "        # Create labels directory\n",
    "        labelsTrain_path = data_path_dir/name/\"labels\"\n",
    "        labelsTrain_path_supcat = data_path_dir/name/\"labels_supCat\"\n",
    "        labelsTrain_path_supcat.mkdir(exist_ok=True)\n",
    "        labelsTrain_path.mkdir(exist_ok=True, parents=True)\n",
    "        #Get data from annotations json file\n",
    "        with open(path_annotations, \"r\", encoding=\"utf-8\") as f:\n",
    "            annotates = json.load(f)\n",
    "        #Create DataFrame from data json file\n",
    "        images = pd.DataFrame(annotates[\"images\"], columns=[\"id\",\"file_name\",\"width\",\"height\"])\n",
    "        images.rename(columns={\"id\":\"image_id\"}, inplace=True)\n",
    "        annot = pd.DataFrame(annotates[\"annotations\"], columns=[\"id\",\"image_id\",\"category_id\",\"segmentation\",\"bbox\"])\n",
    "        cat = pd.DataFrame(annotates[\"categories\"]).rename(columns={\"id\":\"category_id\"}).sort_values(by=\"category_id\", ascending=True)\n",
    "        df = annot.merge(images)\n",
    "        df = df.merge(cat)\n",
    "    \n",
    "        cat_index = cat.supercategory.unique()\n",
    "        df.supercategory_id = np.nan\n",
    "\n",
    "        for i,v in enumerate(cat_index):\n",
    "            df.loc[df['supercategory']==v, \"supercategory_id\"]= i\n",
    "        # Loop to create  labels txt files\n",
    "        for img in df[\"image_id\"].unique():        \n",
    "            seg = df[df[\"image_id\"]==img]\n",
    "            length = len(seg.index)\n",
    "            i = 0\n",
    "            name_file = Path(seg['file_name'].values[0])\n",
    "            labelsTrain_supcat = labelsTrain_path_supcat/f\"{name_file.stem}.txt\"\n",
    "            path_txt = labelsTrain_path/f\"{name_file.stem}.txt\"\n",
    "            for j in range(length):\n",
    "                labels_seg = seg.iloc[i:j+1,:]\n",
    "                seg_value = labels_seg['segmentation'].values[0][0]\n",
    "                seg_zn = normalize_segmentation(labels_seg)\n",
    "                #print(\"values ===>\",seg_value)\n",
    "                coord_seg = \",\".join([str(x)for x in seg_zn]).replace(\",\",\" \")\n",
    "                lab_seg = f\"{labels_seg['category_id'].values[0]} {coord_seg}\\n\"\n",
    "                #print(labels_seg[\"bbox\"])\n",
    "                bbox = labels_seg[\"bbox\"].values[0]\n",
    "                bbox_nz = normalise_bbox(labels_seg)\n",
    "\n",
    "                #print(bbox_nz)\n",
    "                #print(labels_seg[\"width\"])\n",
    "                coord_bbox = \",\".join([str(x) for x in bbox_nz]).replace(\",\",\" \")\n",
    "                lab_bbox = f\"{int(labels_seg['supercategory_id'].values[0])} {coord_bbox}\\n\"\n",
    "                i+=1\n",
    "                with open(labelsTrain_supcat,\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(lab_bbox)\n",
    "\n",
    "                with open(path_txt,\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(lab_seg)\n",
    "    print(\"==================finished===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================finished===========================\n"
     ]
    }
   ],
   "source": [
    "build_labels_txt_with_segmentations(datasets,[\"train\",\"test\",\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\repositoryProf\\Project_E2\\TACO\\data\\annotations_0_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    get = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = get[\"annotations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/notebooks')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml_taco_data():\n",
    "    cur_dir = Path.cwd()\n",
    "    taco_data = cur_dir/\"TACO\"/\"data\"\n",
    "    yolov7_dir = cur_dir/\"yolov7\"\n",
    "    yolov7_data = yolov7_dir/\"data\"\n",
    "    yaml_data = yolov7_data/\"taco.yaml\"\n",
    "    yaml_data_sup = yolov7_data/\"taco_sup.yaml\"\n",
    "    global_annotations_path = [f for f in taco_data.iterdir() if str(f).endswith(\"annotations.json\")][0]\n",
    "    with open(global_annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        global_annotations = json.load(f)\n",
    "        categories = pd.DataFrame(global_annotations[\"categories\"]).rename(columns={\"id\":\"category_id\"}).sort_values(by=\"category_id\")\n",
    "        annotations_global = pd.DataFrame(global_annotations[\"annotations\"],columns=[\"category_id\"]).merge(categories).sort_values(by=\"category_id\")\n",
    "        \n",
    "    classes = list(categories[\"name\"].unique())\n",
    "    nc = len(categories[\"name\"].unique())\n",
    "    nc_sup = len(categories[\"supercategory\"].unique())\n",
    "    classes_sup = list(categories[\"supercategory\"].unique())\n",
    "    dict_taco_yaml={\n",
    "        \"train\": \"../datasets/train/images\",\n",
    "        \"val\": \"../datasets/val/images\",\n",
    "        \"test\": \"../datasets/test/images\",\n",
    "        \n",
    "        \"nc\": nc,\n",
    "        \"names\":classes,    \n",
    "    }\n",
    "    dict_taco_sup_yaml={\n",
    "        \"train\": \"../datasets/train/images\",\n",
    "        \"val\": \"../datasets/val/images\",\n",
    "        \"test\": \"../datasets/test/images\",\n",
    "        \n",
    "        \"nc\": nc_sup,\n",
    "        \"names\":classes_sup,    \n",
    "    }\n",
    "    with open(yaml_data_sup, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(dict_taco_sup_yaml,f, indent=4)\n",
    "    with open(yaml_data, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(dict_taco_yaml,f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yaml_taco_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f041d930f62173bda57140748705d746e0dd4225ea085e215e0166a72756a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
